Pods and container 
=====================

- Managing Application Configuration
- Managing Container Resources
- Monitoring Container Health with Probes
- Building Self-Healing Pods with Restart Policies


Managing Application Configuration
====================================

We will see about managing application configuration.


- Application Configuration
- ConfigMap
- Secrets  ( which will allow us to store sensitive configuration data)
- Environment Variable
- Configuration Volume


what is application configuration?
--------------------------------

When you're running an application in Kubernetes, you will probably, at some point, want to pass dynamic values to your applications at runtime in order to control how they behave. This process is known as application configuration.

So we're just passing data to our containers that is going to control what those containers do and how they run.

There are two primary ways to store configuration data in Kubernetes, and the first one is the ConfigMap and second one is Secrets.

ConfigMap
---------

ConfigMaps allow us to store data in the form of a key value map, and we can then pass this configuration data to our container applications.

In ConfigMap definition  Yaml file in the data section, It's a map of key value pairs.
So we have simple key value pairs. and we can have nested key value pairs, and we can even define multiline data.

That multiline data is particularly useful if you want to define something like a configuration file that you then want to make available to one of your
containers. 

Secrets
-------

There is another way to store configuration data in Kubernetes, and that is known as a secret. Secrets are very similar to ConfigMaps,
but they are designed to securely store sensitive data.

So if you have things like passwords or API keys included in your configuration data, you probably want to use a secret instead of a ConfigMap,
just to ensure that that data is stored in a secure way.


So we have our config maps and secrets, but how do we actually take that data and use it? How do we pass that data to our containers?

Here we will use Environment Variable and Configuration Volume

Environment Variable
--------------------

We can pass ConfigMap and secret data to our containers using environment variables, and these variables will be visible at runtime to the container process.

we're defining an environment variable that we are pulling from a particular key within a ConfigMap, and then that is going to be passed as an environment variable to our container process.

Configuration Volume
--------------------

The other way to pass data to our containers is through the use of configuration volumes. With the configuration volume, our configuration data from our ConfigMap or secret is passed in the form of a mounted volume.

It means is that our configuration data is going to appear in the form of files that are available on the container file system.

Here we  have a file for each top-level key in the configuration data, and the contents of that file is going to be all of the data and sub
keys associated with that top-level key.

This is a simple configmap with 2 keys. The first key has the value Hello, world!.  and The second key has a multiline value with just some test data that has multiple lines


kubectl delete configmap  my-configmap  ( To delete the configmap)

This is the declarative way to creating the configmap file

Create a ConfigMap.

vi my-configmap.yml

apiVersion: v1
kind: ConfigMap
metadata:
  name: my-configmap
data:
  key1: Hello, world!
  key2: |
    Test
    multiple lines
    more lines
	

kubectl create -f my-configmap.yml

kubectl get cm

kubectl get configmaps

View your ConfigMap data.

kubectl describe configmap my-configmap

we can actually see the contents of our configuration data using kubectl describe

Secrets
--------

Create a secret.

Get two base64-encoded values.

echo -n 'secret' | base64
echo -n 'anothersecret' | base64



in my data section, I have a couple of keys, but we have not entered the actual data for those keys, and the reason is when we create it via a YAML file 
we need to pass base64 encode our secret values.



Include your two base64-encoded values in the file.

apiVersion: v1
kind: Secret
metadata:
  name: my-secret
type: Opaque
data:
  secretkey1: <base64 String 1>
  secretkey2: <base64 String 2>
  

Lab sesson
----------

 kubectl delete secrets my-secret
 
Execute below 2 lines to get base64-encoded

echo -n 'secret' | base64
echo -n 'anothersecret' | base64

and copy both the values  and paste in my-secret.yml file

c2VjcmV0
YW5vdGhlcnNlY3JldA==


vi my-secret.yml

apiVersion: v1
kind: Secret
metadata:
  name: my-secret
type: Opaque
data:
  secretkey1: c2VjcmV0
  secretkey2: YW5vdGhlcnNlY3JldA==
  

 kubectl create -f my-secret.yml
 
 
 Now we will explore how we can actually pass this configuration data to our pods. Now we passing some configuration data using environment variables.

 
Create a pod and supply configuration data using environment variables.

start with a  basic pod definition running busybox. 

In command section We will do  an  echo ConfigMap:  followed by this CONFIGMAPVAR environment variable. then another SECRETVAR environment variable

Now we can pass many environment variable , here  we are passing two environment variable

First environment variable Under CONFIGMAPVAR

valueFrom  attribute inform to the cluster where the value is coming from.

we will use ConfigMapKeyRef to indicate that we are  going to be referencing a specific key of a ConfigMap. So first we  need to specify the name of the ConfigMap, which is just that one we created earlier, my-configmap, and then the name of the key. So the value of key1 in that ConfigMap is going to be the value of that environment variable.

Second another environment variable Under SECRETVAR

I'm still referencing the name of the secret and the name of the key within the secret. In this case,
it's just going to look for a secret called my-secret ( which i created earlier) and a key called secretkey1.


vi env-pod.yml

apiVersion: v1
kind: Pod
metadata:
  name: env-pod
spec:
  containers:
  - name: busybox
    image: busybox
    command: ['sh', '-c', 'echo "configmap: $CONFIGMAPVAR secret: $SECRETVAR"']
    env:
    - name: CONFIGMAPVAR
      valueFrom:
        configMapKeyRef:
          name: my-configmap
          key: key1
    - name: SECRETVAR
      valueFrom:
        secretKeyRef:
          name: my-secret
          key: secretkey1

kubectl create -f env-pod.yml

Check the log for the pod to see your configuration values!

kubectl logs env-pod


Output:  see Hello world as the value here and then my first secret value that I included for that first key within my-secret is just the word secret.
So we can see that our ConfigMap data and our secret data are in fact being exposed to the container process as environment variables.


configuration volumes
----------------------

About exposing our data to the container, and that is through the use of configuration volumes.

Create a pod and supply configuration data using volumes.

here we are going to add two configuration volumes

first one is  to be called configmap-volume and pulling it from a ConfigMap. 
this key does is it tells the cluster to pull the contents of this volume from a ConfigMap, and the name of that ConfigMap is going to be my-configmap (which is created earlier)

same thing with the secret. Instead of using the ConfigMap key here, we're using secret here. So it's going to pull from a secret called my-secret. (which is created earlier)

we've set up those volumes in our pod specification, we need to add them as volume mounts to the container so that the container can see the contents of those volumes.

So we are  going to mount that configmap-volume and the secret-volume and the
configmap-volume is going to be mounted to etc/config/configmap.

So inside that directory, inside the container,
we should see our ConfigMap data, and similarly in etc/config/secret, we should see our secret data inside the container.

vi volume-pod.yml


apiVersion: v1
kind: Pod
metadata:
  name: volume-pod
spec:
  containers:
  - name: busybox
    image: busybox
    command: ['sh', '-c', 'while true; do sleep 3600; done']
    volumeMounts:
    - name: configmap-volume
      mountPath: /etc/config/configmap
    - name: secret-volume
      mountPath: /etc/config/secret
  volumes:
  - name: configmap-volume
    configMap:
      name: my-configmap
  - name: secret-volume
    secret:
      secretName: my-secret


kubectl create -f volume-pod.yml

Use kubectl exec to navigate inside the pod and see your mounted config data files.

kubectl exec volume-pod -- ls /etc/config/configmap  ( We can actually see two files here: key1 and key2. So those are the two top-level keys in that ConfigMap)

kubectl exec volume-pod -- cat /etc/config/configmap/key1 ( look at the contents of key1 , our value there was just Hello world)
kubectl exec volume-pod -- cat /etc/config/configmap/key2  ( look at the contents of key2 , multiline piece of data in our ConfigMap )

kubectl exec volume-pod -- ls /etc/config/secret ( Here we can see we have our two top-level keys there and inside that secretkey1 and secretkey2 file

kubectl exec volume-pod -- cat /etc/config/secret/secretkey1  ( Inside secretkey1 file we have a word called "secret")
kubectl exec volume-pod -- cat /etc/config/secret/secretkey2  ( Inside secretkey2 file we have a word called "another secret")
		  

UseCase -1 ( Passing Configuration Data to a Kubernetes Container)
==================================================================

A company that provides regular shipments  to customers. The company is working on deploying some applications to a Kubernetes cluster.

One of these applications is a simple Nginx web server. This server is used as part of a secure backend application, and the company would like it to be configured to use HTTP basic authentication.

This will require an htpasswd file as well as a custom Nginx config file. In order to deploy this Nginx server to the cluster with good configuration practices, you will need to load the custom Nginx configuration from a ConfigMap (this already exists) and use a Secret to store the htpasswd data.

Create a Pod with a container running the nginx:1.19.1 image. Supply a custom Nginx configuration using a ConfigMap, and populate an htpasswd file using a Secret.

htpasswd is already installed on the server, and you can generate an htpasswd file like so:

htpasswd -c .htpasswd user
A pod called busybox already exists in the cluster, which you can use to contact your Nginx pod and test your setup.


Kubernetes has multiple options for storing and managing configuration data. 

Here will focus on the process of passing that configuration data to your containers in order to configure applications. 
To work with application configuration in Kubernetes  passing some existing configuration data stored in Secrets and ConfigMaps to a container.

Generate an htpasswd File and Store It as a Secret

To install htpasswd in ubuntu machine

apt-get update
sudo apt install apache2-utils


Generate an htpasswd file:

htpasswd -c .htpasswd user

Create a password you can easily remember (we'll need it again later). - test1234

ls -la ( we can see the .htpasswd file)

View the file's contents:

cat .htpasswd

Create a Secret containing the htpasswd data:

kubectl create secret generic nginx-htpasswd --from-file .htpasswd

Delete the .htpasswd file:

rm .htpasswd

Create the Nginx Pod
Create pod.yml:

vi pod.yml
Set vi to 'paste' mode by hitting :, and then typing set paste (ENTER). Then switch back to INSERT mode by hitting i.

Enter the following to create the pod and mount the Nginx config and htpasswd Secret data:


apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.19.1
    ports:
    - containerPort: 80
    volumeMounts:
    - name: config-volume
      mountPath: /etc/nginx
    - name: htpasswd-volume
      mountPath: /etc/nginx/conf
  volumes:
  - name: config-volume
    configMap:
      name: nginx-config
  - name: htpasswd-volume
    secret:
      secretName: nginx-htpasswd



Save and exit the file by pressing Escape followed by :wq.

View the existing ConfigMap:

kubectl get cm
Get more info about nginx-config:

kubectl describe cm nginx-config

Here we can see  nginx.conf 

Create the pod:

kubectl apply -f pod.yml
Check the status of your pod and get its IP address:

kubectl get pods -o wide
Its IP address will be listed once it has a Running status. We'll use this in the final two commands.

Within the existing busybox pod, without using credentials, verify everything is working:

kubectl exec busybox -- curl 192.168.194.91

kubectl exec busybox -- curl <NGINX_POD_IP>
We'll see HTML for the 401 Authorization Required page â€” but this is a good thing, as it means our setup is working as expected.

Run the same command again using credentials (including the password you created at the beginning ) to verify everything is working:

kubectl exec busybox -- curl -u user:<PASSWORD> <NGINX_POD_IP>
This time, we'll see the Welcome to nginx! page HTML.



Managing Container Resourcees
==============================
Here two thinngs we need to understand

- resource requests
- resource limits

Resourcees requests
-------------------

resource requests allow you to define an amount of resources things like CPU and memory.

Resource requests allow us to define the amount of resources that we might expect our container to use.

it allows the Kubernetes scheduler to take that into account when it is scheduling pods on our different nodes.

Kubernetes scheduler is going to look at those resource requests, and if your resource request is asking for five gigabytes of memory,
and there's not five gigabytes of memory available on a particular node, the Kubernetes scheduler is going to look for a different node that does have
five gigabytes of memory available before scheduling that particular pod.
It's important to note that containers are allowed to use more or less than the requested amount of resources.


The resource requests does not actually force the container to stay within that
limit. Resource requests really only affect scheduling.

So resource requests basically allow us to give the Kubernetes cluster an
estimate of how much resources we think our containers are going to use

Resource limits
================

Resource Limit actually provide us a way to limit the amount of resources our containers can use and stop our containers from using more resources than they should. 

It's important to note that when you use resource limits, the container runtime is responsible for enforcing resource limits and different container runtimes do this in different ways. 

For example, some runtimes might enforce resource limits by terminating the container process. So depending on which container runtime you're using.

if you're setting resource limits, your containers may actually get stopped if they attempt to use more resources than are specified in the resource limit.



vi big-request-pod.yml

apiVersion: v1
kind: Pod
metadata:
  name: big-request-pod
spec:
  containers:
  - name: busybox
    image: busybox
    command: ['sh', '-c', 'while true; do sleep 3600; done']
    resources:
      requests:
        cpu: "10000m"
        memory: "128Mi"

kubectl create -f big-request-pod.yml

Check the pod status. It should never leave the Pending state since no worker nodes have enough resources to meet the
request.

kubectl get pod big-request-pod

Create a pod with resource requests and limits.

vi resource-pod.yml

	
apiVersion: v1
kind: Pod
metadata:
  name: resource-pod
spec:
  containers:
  - name: busybox
    image: busybox
    command: ['sh', '-c', 'while true; do sleep 3600; done']
    resources:
      requests:
        cpu: "250m"
        memory: "128Mi"
      limits:
        cpu: "500m"
        memory: "256Mi"	

kubectl create -f resource-pod.yml

kubectl get pod resource-pod
Check the pod status.

kubectl get pods



Monitoring Container Health with Probes
=======================================	
Probes are part of the container spec in Kubernetes.They allow you to customize how kubernetes detect the state of a container.

There are 3 types of Probes.

We will discuss about what container health is and about Kubernetes probes starting with liveness probes, then startup probes, then readiness probes.

Kubernetes provides a number of features that allow us to build robust applications, and one of those features is the ability to automatically restart unhealthy containers. 

So to make the most of these features, Kubernetes needs to be able to accurately determine the status of our applications.



we're checking to see if our containers are running well as expected, if they are functioning correctly, or if they're broken.

If they're functioning correctly, they're healthy. If something's wrong, then our containers are unhealthy,

one of the biggest tools in our toolbox for dealing with container health is the liveness probe.

Liveness probes
---------------

Liveness probes allow us to automatically determine whether or not our container application is in a healthy state. By default,
Kubernetes will only consider a container to be down or broken if the container process stops.

but liveness probes can allow us to customize this detection mechanism and make it more sophisticated. Sometimes your containers might still be running.

The process might not have stopped, but things are still broken.

With liveliness probes, we can detect those more sophisticated, more subtle situations where things aren't working, but the container is still running.



startup probes
--------------

These are very similar to liveliness probes,but liveliness probes run constantly on a schedule and startup probes only run
at container startup and stop once they succeed.

Essentially startup probes are used to detect when the application has successfully started up.

Their way of monitoring container health during the startup process, whereas liveness probes monitor container health on an ongoing basis.

Startup probes are especially useful for legacy applications that might have very long startup times.

Readiness probe
---------------

Readiness probes are used to determine when a container is ready to accept requests. Like startup probes, they run during the startup process rather than on an ongoing basis,but they do serve a slightly different purpose.

When you have a service that is backed by multiple containers, you could end up in a situation where a container is still starting up, but user traffic is being sent to that container.

Therefore users are getting a bad experience or something isn't working just because that container is still starting up.

So what readiness probes do is they detect when the container is fully started up and they prevent user traffic from being sent to that particular pod
until that point.
So until your containers are fully started up and the readiness probe is able to detect that fact,users are not actually using that pod or container.


Create a pod with a command-based liveness probe.

vi liveness-pod.yml


apiVersion: v1
kind: Pod
metadata:
  name: liveness-pod
spec:
  containers:
  - name: busybox
    image: busybox
    command: ['sh', '-c', 'while true; do sleep 3600; done']
    livenessProbe:
      exec:
        command: ["echo", "Hello, world!"]
      initialDelaySeconds: 5
      periodSeconds: 5


kubectl create -f liveness-pod.yml

Check the pod status

kubectl get pod liveness-pod

Create a pod with an http-based liveness probe

vi liveness-pod-http.yml


apiVersion: v1
kind: Pod
metadata:
  name: liveness-pod-http
spec:
  containers:
  - name: nginx
    image: nginx:1.19.1
    livenessProbe:
      httpGet:
        path: /
        port: 80
      initialDelaySeconds: 5
      periodSeconds: 5


kubectl create -f liveness-pod-http.yml

Check the pod status.

kubectl get pod liveness-pod-http

kubectl describe pod liveness-pod-http

Create a pod with a startup probe.

vi startup-pod.yml

apiVersion: v1
kind: Pod
metadata:
  name: startup-pod
spec:
  containers:
  - name: nginx
    image: nginx:1.19.1
    startupProbe:
      httpGet:
        path: /
        port: 80
      failureThreshold: 30
      periodSeconds: 10




kubectl create -f startup-pod.yml

Check the pod status.

kubectl get pod startup-pod


Create a pod with a readiness probe.

vi readiness-pod.yml

apiVersion: v1
kind: Pod
metadata:
  name: readiness-pod
spec:
  containers:
  - name: nginx
    image: nginx:1.20.1
    ports:
    - containerPort: 80
    livenessProbe:
      httpGet:
        path: /
        port: 80
      initialDelaySeconds: 3
      periodSeconds: 3
    readinessProbe:
      httpGet:
        path: /
        port: 80
      initialDelaySeconds: 15
      periodSeconds: 5

	  
-------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: readiness-pod
spec:
  containers:
  - name: nginx
    image: nginx:1.19.1
    readinessProbe:
      httpGet:
        path: /
        port: 80
      initialDelaySeconds: 15
      periodSeconds: 5
---------------------------------------------
kubectl create -f readiness-pod.yml

Check the pod status.

kubectl get pod readiness-pod	  ( pod is running but ready say 0/1. container is up and running but it is not ready.Because readiness probe  has not  completed yet.)

After 15 second  , run again pod is ready 1/1)


Building Self-Healing Pods with Restart Policies
=================================================

This is about building self-healing pods with restart policies.

We have three restart policies that exist in Kubernetes

- Always
- OnFailure
- Never

Kubernetes can automatically restart containers when they fail, and we know about using probes to detect container failures.

Restart policies, basically control what happens once we detect that a container is unhealthy.

Restart policies allow you to customize the behavior that occurs when there's an unhealthy container,
so you're able to define when your containers are automatically restarted or even say that you don't want your containers to be automatically restarted.

Restart policies are an important component of self-healing applications. These applications are automatically repaired when a problem arises,
and we automatically repairing applications is simply by restarting them. So restart policies play a big role in that process.

There are three possible restart policies in Kubernetes: Always, OnFailure, and Never. 

Always
------

The Always restart policy is the default restart policy. So if you don't define  any restart policy , this is the one that's going to take effect.
With this policy, containers will always be restarted if they stop or become unhealthy.

So if a container runs to completion and completes successfully, that means it exits with a successful exit code.
The container is going to be restarted in that case.

Or if the container fails, if it becomes unhealthy or it exits with an error code, it'll be restarted in that case as well. 

So no matter what happens, if the container stops or becomes unhealthy, it's going to be restarted.

You should use this policy for any application that should essentially just always be running.

OnFailure
---------

OnFailure will restart the container process. Only if the container process becomes unhealthy or exits with an error code.

So if you have some software that is designed to run once to a successful completion and then stop, and it doesn't need to be run again, you want to use the OnFailure policy. 

It is going to automatically restart the application if it fails, but if it succeeds and finishes, it won't be restarted.

Never
----- 
Never restart policy which is essentially the opposite of the Always restart policy.

No matter what happens, if a container completes successfully, or if it completes unsuccessfully, it will never be restarted.

You should use this for applications that should only run once and never be automatically restarted.


Create a pod with the Always restart policy that completes after a few second

vi always-pod.yml

apiVersion: v1
kind: Pod
metadata:
  name: always-pod
spec:
  restartPolicy: Always
  containers:
  - name: busybox
    image: busybox
    command: ['sh', '-c', 'sleep 10']


kubectl create -f always-pod.yml

Check the pod status. You should see it restarting after every 10-second completion

kubectl get pod always-pod


Create a pod with the OnFailure restart policy	

vi onfailure-pod.yml


apiVersion: v1
kind: Pod
metadata:
  name: onfailure-pod
spec:
  restartPolicy: OnFailure
  containers:
  - name: busybox
    image: busybox
    command: ['sh', '-c', 'sleep 10']
	

kubectl create -f onfailure-pod.yml

Check the pod status. Note that the pod does not restart because it completed successfully

kubectl get pod onfailure-pod


Delete, modify, and recreate the pod so that it fails

kubectl delete pod  onfailure-pod

Now create the below pod again.

vi onfailure-pod.yml

apiVersion: v1
kind: Pod
metadata:
  name: onfailure-pod
spec:
  restartPolicy: OnFailure
  containers:
  - name: busybox
    image: busybox
    command: ['sh', '-c', 'sleep 10; this is a bad command that will fail']
	

kubectl create -f onfailure-pod.yml

Check the pod status. Note that the pod restarts because it exited with an error code.

kubectl get pod onfailure-pod



Create a pod with the Never restart policy that completes after a few seconds.

vi never-pod.yml

apiVersion: v1
kind: Pod
metadata:
  name: never-pod
spec:
  restartPolicy: Never
  containers:
  - name: busybox
    image: busybox
    command: ['sh', '-c', 'sleep 10; this is a bad command that will fail']
	

kubectl create -f never-pod.yml

Check the pod status. You should see that it does not restart after completing.

kubectl get pod never-pod	
	



init containers
================

what are init containers?

init containers are essentially containers that run once to completion during the startup process of a pod.

So these are not your main application containers that are running on an ongoing basis. They just run once as the pod is starting up.
A pod can have any number of init containers and each of those init containers is going to run once in order to completion.	